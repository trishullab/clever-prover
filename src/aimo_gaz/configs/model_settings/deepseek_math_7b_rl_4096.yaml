name_or_path: deepseek-ai/deepseek-math-7b-rl
logging_dir: <AIMO_GAZ_ROOT>/.logs/model
use_vllm: False
vllm_model_args:
  dtype: half
  tensor_parallel_size: 2
vllm_sample_args:
  temperature: 0.8
  top_p: 0.95
model_args:
  no_init_eval: True
  padding: True
  truncation: True
  max_seq_length: 4096
  max_length: 4096
  load_model: True
  use_lora: True
  is_seq2seq: False # Deepseek is Causal LM
  token:
  comet_experiment:
  base_device: 0
  device_map:
    model.embed_tokens : 0
    model.layers.0 : 0
    model.layers.1 : 0
    model.layers.2 : 0
    model.layers.3 : 0
    model.layers.4 : 0
    model.layers.5 : 0
    model.layers.6 : 0
    model.layers.7 : 0
    model.layers.8 : 0
    model.layers.9 : 0
    model.layers.10 : 0
    model.layers.11 : 0
    model.layers.12 : 0
    model.layers.13 : 0
    model.layers.14 : 0
    model.layers.15 : 0
    model.layers.16 : 0
    model.layers.17 : 0
    model.layers.18 : 1
    model.layers.19 : 1
    model.layers.20 : 1
    model.layers.21 : 1
    model.layers.22 : 1
    model.layers.23 : 1
    model.layers.24 : 1
    model.layers.25 : 1
    model.layers.26 : 1
    model.layers.27 : 1
    model.layers.28 : 1
    model.layers.29 : 1
    model.norm : 1
    lm_head : 1