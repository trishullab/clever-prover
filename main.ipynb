{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":73231,"databundleVersionId":8365361},{"sourceType":"datasetVersion","sourceId":8707489,"datasetId":5223138,"databundleVersionId":8860510},{"sourceType":"datasetVersion","sourceId":8052555,"datasetId":4748944,"databundleVersionId":8166794},{"sourceType":"datasetVersion","sourceId":8771554,"datasetId":5263819,"databundleVersionId":8927345},{"sourceType":"datasetVersion","sourceId":8799183,"datasetId":5223302,"databundleVersionId":8956385},{"sourceType":"datasetVersion","sourceId":8149693,"datasetId":4819800,"databundleVersionId":8270829},{"sourceType":"datasetVersion","sourceId":7369493,"datasetId":4281572,"databundleVersionId":7460056},{"sourceType":"datasetVersion","sourceId":8023365,"datasetId":4728129,"databundleVersionId":8135930},{"sourceType":"datasetVersion","sourceId":7867367,"datasetId":4615754,"databundleVersionId":7972658},{"sourceType":"datasetVersion","sourceId":4086339,"datasetId":2418291,"databundleVersionId":4142499},{"sourceType":"datasetVersion","sourceId":4053053,"datasetId":2398033,"databundleVersionId":4109087},{"sourceType":"datasetVersion","sourceId":8706919,"datasetId":5222750,"databundleVersionId":8859902},{"sourceType":"datasetVersion","sourceId":8708282,"datasetId":5223654,"databundleVersionId":8861337},{"sourceType":"datasetVersion","sourceId":8707395,"datasetId":5223082,"databundleVersionId":8860411},{"sourceType":"datasetVersion","sourceId":8708340,"datasetId":5223698,"databundleVersionId":8861399}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":272.023448,"end_time":"2024-06-22T22:29:29.890355","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-22T22:24:57.866907","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0da7620dc2854885a88d3c5aca1f8de1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e51797acbeda47ee9f0984077c43c9ad","placeholder":"​","style":"IPY_MODEL_58f367d350be45128556252946c4d090","value":" 3/3 [01:39&lt;00:00, 32.21s/it]"}},"114dc8b22e544c84b2547b8a1a076512":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f97b0ce8b2e416ebc8e58fa6eccafac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_504d7e78f31b426ab38ce7a58b74ac0d","placeholder":"​","style":"IPY_MODEL_114dc8b22e544c84b2547b8a1a076512","value":"Loading checkpoint shards: 100%"}},"281867fd08c44bb6b7bf8ecc5b8b389b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f97b0ce8b2e416ebc8e58fa6eccafac","IPY_MODEL_f90075bba4b84f49ba6cb53a64750fed","IPY_MODEL_0da7620dc2854885a88d3c5aca1f8de1"],"layout":"IPY_MODEL_a207bc4c0e2b48e0ab0eefd5d80215bd"}},"504d7e78f31b426ab38ce7a58b74ac0d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58f367d350be45128556252946c4d090":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93b3fad2238a4708b56b9c20b2a8e05f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a207bc4c0e2b48e0ab0eefd5d80215bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdb47d6fd1fa4177a7e453afde09c770":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e51797acbeda47ee9f0984077c43c9ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f90075bba4b84f49ba6cb53a64750fed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93b3fad2238a4708b56b9c20b2a8e05f","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cdb47d6fd1fa4177a7e453afde09c770","value":3}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\ntime_start = time.time()\nimport os\nimport torch\nimport gc\nimport math\nON_KAGGLE = os.path.exists('/kaggle/input')\nENABLE_SUBMISSION = True\nGENERATE_WHL = False\nos.environ[\"ON_KAGGLE\"] = str(ON_KAGGLE)\nenv = None\nos.environ[\"USE_VLLM\"] = \"False\"\n# Clear cache, helps in re-running the notebook\ngc.collect()\ntorch.cuda.empty_cache()\n# This is related to T4 GPU on collab\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"papermill":{"duration":0.01842,"end_time":"2024-06-22T22:25:01.790016","exception":false,"start_time":"2024-06-22T22:25:01.771596","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-27T06:49:00.971017Z","iopub.execute_input":"2024-06-27T06:49:00.971396Z","iopub.status.idle":"2024-06-27T06:49:04.370163Z","shell.execute_reply.started":"2024-06-27T06:49:00.971360Z","shell.execute_reply":"2024-06-27T06:49:04.369110Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Install required packages if on Kaggle\nif ON_KAGGLE:\n    !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/omegaconf222py3/antlr4_python3_runtime-4.9.3-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/omegaconf222py3/omegaconf-2.2.2-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/hydracore120py3/hydra_core-1.2.0-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/parglare-0-18-0/parglare-0.18.0-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/rank-bm25-whl/rank_bm25-0.2.2-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/huggingface-trl-package/shtab-1.7.1-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/huggingface-trl-package/tyro-0.7.3-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/peft-0-11-1/peft-0.11.1-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/huggingface-trl-package/trl-0.7.11-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/amino-gaz-whl/aimo_gaz-0.0.1-py3-none-any.whl -qq --no-deps --force-reinstall\nelse:\n    !pip install -r requirements.txt\n    !pip install -e . --force-reinstall\nif GENERATE_WHL:\n    !pip install --upgrade build\n    !python -m build","metadata":{"papermill":{"duration":112.26536,"end_time":"2024-06-22T22:26:54.058526","exception":false,"start_time":"2024-06-22T22:25:01.793166","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-27T06:49:04.372437Z","iopub.execute_input":"2024-06-27T06:49:04.373151Z","iopub.status.idle":"2024-06-27T06:55:27.560854Z","shell.execute_reply.started":"2024-06-27T06:49:04.373118Z","shell.execute_reply":"2024-06-27T06:55:27.559744Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import time\nfrom aimo_gaz.solver.solver_config import parse_solver_config\nfrom aimo_gaz.tools.log_utils import setup_logger\nfrom hydra import compose, initialize\n\nconfig_path = '/kaggle/input/aimo-gaz-configs/configs' if ON_KAGGLE else 'src/aimo_gaz/configs'\nconfig_name = 'coordination_solver_config.yaml'\n\n# Setup the logger\nif \"AIMO_GAZ_ROOT\" not in os.environ:\n    os.environ[\"AIMO_GAZ_ROOT\"] = '/kaggle/working' if ON_KAGGLE else 'src/aimo_gaz'\nlog_dir_name = \"logs\" if ON_KAGGLE else '.logs'\nroot_dir = os.environ[\"AIMO_GAZ_ROOT\"]\n# Check if the configs exists in the root directory\nif not os.path.exists(os.path.join(root_dir, 'configs')):\n    os.makedirs(os.path.join(root_dir, 'configs'))\n# Copy the configs to the root directory\nos.system(f'cp -r {config_path}/* {os.path.join(root_dir, \"configs\")}')\n# Create the logs directory\nif not os.path.exists(os.path.join(root_dir, log_dir_name)):\n    os.makedirs(os.path.join(root_dir, log_dir_name), exist_ok=True)\ntime_str = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\nos.makedirs(os.path.join(root_dir, log_dir_name, time_str), exist_ok=True)\nlogger = setup_logger(\"aimo_gaz\", os.path.join(root_dir, log_dir_name, time_str, 'aimo_gaz.log'))\nlogger.info(f\"Using config file: {config_name}\")\nwith initialize(version_base=\"1.2\", config_path=\"configs\", job_name=\"aimo_gaz\"):\n    cfg = compose(config_name=config_name) # Load the config file\n    solver_config = parse_solver_config(cfg)\nsolver = solver_config.get_solver(logger=logger)\nassert solver is not None, \"Solver not initialized\"\n    ","metadata":{"papermill":{"duration":33.955339,"end_time":"2024-06-22T22:27:28.017009","exception":false,"start_time":"2024-06-22T22:26:54.061670","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-27T06:55:27.562306Z","iopub.execute_input":"2024-06-27T06:55:27.562581Z","iopub.status.idle":"2024-06-27T06:55:44.935676Z","shell.execute_reply.started":"2024-06-27T06:55:27.562556Z","shell.execute_reply":"2024-06-27T06:55:44.934658Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-06-27 06:55:31.433908: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-27 06:55:31.434001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-27 06:55:31.593358: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'planner_solver_config.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n  warnings.warn(msg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'execution_solver_config.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n  warnings.warn(msg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'code_solver_config.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n  warnings.warn(msg, UserWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set up the evaluation API\nif ENABLE_SUBMISSION and ON_KAGGLE:\n    import aimo\n    if env is None:\n        env = aimo.make_env()\n    iter_test = env.iter_test()","metadata":{"papermill":{"duration":0.036219,"end_time":"2024-06-22T22:27:28.056329","exception":false,"start_time":"2024-06-22T22:27:28.020110","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-27T06:55:44.937076Z","iopub.execute_input":"2024-06-27T06:55:44.937407Z","iopub.status.idle":"2024-06-27T06:55:44.959354Z","shell.execute_reply.started":"2024-06-27T06:55:44.937380Z","shell.execute_reply":"2024-06-27T06:55:44.958640Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if ENABLE_SUBMISSION and ON_KAGGLE:\n    prep_time = math.ceil(time.time() - time_start)\n    total_time_left = 9 * 60 * 60 - prep_time - 100 # 9 hours in seconds - time taken for preparation and grace time of 100s\n    test_problems = set([\"What is $1-1$?\", \"What is $0\\times10$?\", \"Solve $4+x=4$ for $x$.\"])\n    reduced_time_for_test_problems = 120\n    with solver:\n        problems_attempted = 0\n        total_problems = 50\n        # Iterate through the test set and use the model make predictions\n        for test, sample_submission in iter_test:\n            start_timer = time.time()\n            print(\"Problem statement:\\n\")\n            problem_statement = test['problem'][0]\n            print(problem_statement)\n            if problem_statement in test_problems:\n                time_allowed_to_solve = reduced_time_for_test_problems\n            else:\n                time_allowed_to_solve = total_time_left//(total_problems - problems_attempted)\n            sample_submission['answer'] = solver.solve(problem_statement, time_allowed = time_allowed_to_solve)\n            env.predict(sample_submission)\n            print(\"Submission:\\n\")\n            print(sample_submission, '\\n')\n            print(\"=\"*50)\n            time_to_attempt = math.ceil(time.time() - start_timer)\n            total_time_left -= time_to_attempt\n            problems_attempted += 1\n            if total_time_left <= 0:\n                break","metadata":{"papermill":{"duration":118.880015,"end_time":"2024-06-22T22:29:26.939316","exception":false,"start_time":"2024-06-22T22:27:28.059301","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-27T06:55:44.961225Z","iopub.execute_input":"2024-06-27T06:55:44.961497Z","iopub.status.idle":"2024-06-27T07:06:23.190217Z","shell.execute_reply.started":"2024-06-27T06:55:44.961474Z","shell.execute_reply":"2024-06-27T07:06:23.188678Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24a25c50f9d546d190e406c221b5a95b"}},"metadata":{}},{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Problem statement:\n\nWhat is $1-1$?\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcess Process-2:\nValueError: Error from parse_expr with transformed code: \"Symbol ('Execution' )timed Symbol ('out' )Symbol ('after' )Float ('10.0' )Symbol ('seconds' ).\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/core/sympify.py\", line 493, in sympify\n    expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py\", line 1090, in parse_expr\n    raise e from ValueError(f\"Error from parse_expr with transformed code: {code!r}\")\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py\", line 1081, in parse_expr\n    rv = eval_expr(code, local_dict, global_dict)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py\", line 909, in eval_expr\n    expr = eval(\n  File \"<string>\", line 1\n    Symbol ('Execution' )timed Symbol ('out' )Symbol ('after' )Float ('10.0' )Symbol ('seconds' ).\n                         ^^^^^\nSyntaxError: invalid syntax\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/aimo_gaz/solver/coordination_solver.py\", line 67, in _run_simplify\n    context[\"simp_output\"] = simplify(output)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/simplify/simplify.py\", line 588, in simplify\n    expr = sympify(expr, rational=rational)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/core/sympify.py\", line 495, in sympify\n    raise SympifyError('could not parse %r' % a, exc)\nsympy.core.sympify.SympifyError: Sympify of expression 'could not parse 'Execution timed out after 10.0 seconds.'' failed, because of exception being raised:\nSyntaxError: invalid syntax (<string>, line 1)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcess Process-4:\nValueError: Error from parse_expr with transformed code: \"Symbol ('Execution' )timed Symbol ('out' )Symbol ('after' )Float ('10.0' )Symbol ('seconds' ).\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/core/sympify.py\", line 493, in sympify\n    expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py\", line 1090, in parse_expr\n    raise e from ValueError(f\"Error from parse_expr with transformed code: {code!r}\")\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py\", line 1081, in parse_expr\n    rv = eval_expr(code, local_dict, global_dict)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py\", line 909, in eval_expr\n    expr = eval(\n  File \"<string>\", line 1\n    Symbol ('Execution' )timed Symbol ('out' )Symbol ('after' )Float ('10.0' )Symbol ('seconds' ).\n                         ^^^^^\nSyntaxError: invalid syntax\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/aimo_gaz/solver/coordination_solver.py\", line 67, in _run_simplify\n    context[\"simp_output\"] = simplify(output)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/simplify/simplify.py\", line 588, in simplify\n    expr = sympify(expr, rational=rational)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/core/sympify.py\", line 495, in sympify\n    raise SympifyError('could not parse %r' % a, exc)\nsympy.core.sympify.SympifyError: Sympify of expression 'could not parse 'Execution timed out after 10.0 seconds.'' failed, because of exception being raised:\nSyntaxError: invalid syntax (<string>, line 1)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcess Process-6:\nValueError: Error from parse_expr with transformed code: \"Symbol ('Execution' )timed Symbol ('out' )Symbol ('after' )Float ('10.0' )Symbol ('seconds' ).\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/core/sympify.py\", line 493, in sympify\n    expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py\", line 1090, in parse_expr\n    raise e from ValueError(f\"Error from parse_expr with transformed code: {code!r}\")\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py\", line 1081, in parse_expr\n    rv = eval_expr(code, local_dict, global_dict)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py\", line 909, in eval_expr\n    expr = eval(\n  File \"<string>\", line 1\n    Symbol ('Execution' )timed Symbol ('out' )Symbol ('after' )Float ('10.0' )Symbol ('seconds' ).\n                         ^^^^^\nSyntaxError: invalid syntax\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/aimo_gaz/solver/coordination_solver.py\", line 67, in _run_simplify\n    context[\"simp_output\"] = simplify(output)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/simplify/simplify.py\", line 588, in simplify\n    expr = sympify(expr, rational=rational)\n  File \"/opt/conda/lib/python3.10/site-packages/sympy/core/sympify.py\", line 495, in sympify\n    raise SympifyError('could not parse %r' % a, exc)\nsympy.core.sympify.SympifyError: Sympify of expression 'could not parse 'Execution timed out after 10.0 seconds.'' failed, because of exception being raised:\nSyntaxError: invalid syntax (<string>, line 1)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m problem_statement \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblem\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(problem_statement)\n\u001b[0;32m---> 13\u001b[0m sample_submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_allowed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_time_left\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtotal_problems\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproblems_attempted\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m env\u001b[38;5;241m.\u001b[39mpredict(sample_submission)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmission:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/aimo_gaz/solver/coordination_solver.py:316\u001b[0m, in \u001b[0;36mCoordinationSolver.solve\u001b[0;34m(self, problem_description, time_allowed)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstragegy \u001b[38;5;241m==\u001b[39m CoordinationSolverStrategy\u001b[38;5;241m.\u001b[39mPLAN_CODE_EXEC_EXRACT_LAST_MAJ_VOTE:\n\u001b[0;32m--> 316\u001b[0m         answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan_code_exec_extract_last_maj_vote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_allowed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStrategy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstragegy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not implemented.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/aimo_gaz/solver/coordination_solver.py:119\u001b[0m, in \u001b[0;36mCoordinationSolver.plan_code_exec_extract_last_maj_vote\u001b[0;34m(self, problem_description, time_allowed)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Plan\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     plan_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 119\u001b[0m     plan \u001b[38;5;241m=\u001b[39m \u001b[43mplanner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_intermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     PLANNER_AVG_TIME \u001b[38;5;241m=\u001b[39m (PLANNER_AVG_TIME \u001b[38;5;241m+\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m plan_start_time)) \u001b[38;5;28;01mif\u001b[39;00m global_attempts \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (PLANNER_AVG_TIME \u001b[38;5;241m*\u001b[39m global_attempts \u001b[38;5;241m+\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m plan_start_time))\u001b[38;5;241m/\u001b[39m(global_attempts \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Code\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/aimo_gaz/solver/planner_solver.py:35\u001b[0m, in \u001b[0;36mPlannerSolver.solve_intermediate\u001b[0;34m(self, problem_description)\u001b[0m\n\u001b[1;32m     33\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m(e)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/aimo_gaz/models/model.py:341\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m num_return_sequences \u001b[38;5;241m=\u001b[39m generate_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m--> 341\u001b[0m     generated_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     generated_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    350\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids, \n\u001b[1;32m    351\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    352\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m    353\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_args)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2437\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[1;32m   2436\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2437\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2439\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}